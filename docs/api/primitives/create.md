# ‚úçÔ∏è Create

**Use the ‚úçÔ∏è Create endpoint to write text according to a provided [prompt](/home/concepts#prompt)**.

Available at ```https://api.lighton.ai/muse/v1/create```.

> üí∏Ô∏è **Pricing**
>
>You will be billed for the **total number of tokens sent in your request plus the number of tokens generated by the API**. Note 
that when using `n_completions` to return multiple possibilities, you will be charged for all of them.

---

## Example


```bash title="Request"
curl -X 'POST' \
  'https://api.lighton.ai/muse/v1/create' \
  -H 'Content-Type: application/json' \
  -H 'Accept: application/json' \
  -H 'X-API-KEY: YOUR_API_KEY' \
  -H 'X-Model: orion-fr' \
  -d '{"text": "Il √©tait une fois", "params": {"mode": "nucleus", "n_tokens": 25, "p": 0.9}}'
```


```json title="Response (JSON)"
{
   "request_id":"969718f5-d1f4-40cd-a872-a6fb7bf84329",
   "outputs":[
      [
         {
            "input_text":"Il √©tait une fois",
            "completions":[
               {
                  "output_text":" une toute petite fille, qui avait une bonne voix, et chantait des chansons am√©ricaines telles que Tabernacle et We all take",
                  "score":-82.67915979400277,
                  "normalized_score":-3.307166391760111,
                  "token_scores":null,
                  "execution_metadata":{
                     "cost":25
                  }
               }
            ],
            "execution_metadata":{
               "cost":25
            }
         }
      ]
   ],
   "total_cost":25
}
```

## Parameters

#### `text` <span class="param-types">string/array[string]</span> <span class="param-warning">‚ö†Ô∏è required</span>

The input(s) that will be used by the model for generation, also known as the prompt. They can be provided either as a single string or as an array of strings for
[batch processing](/api/specifications/requests#batching).

#### `n_tokens` <span class="param-types">int</span> <span class="param-optional">20</span>

Number of [tokens](/home/concepts#tokens) to generate. This can be overridden by a list of `stop_words`, which will cause
generation to halt when a word in such list is encountered.

> ‚ö†Ô∏è **Maximum content length**
>
>Our models can process sequences of **1,024 tokens** at most (length of prompt + `n_tokens`). Requests overflowing this 
maximum length will see their prompt truncated from the left to fit. 


#### `n_completions` <span class="param-types">int</span> <span class="param-optional">1</span>

Number of different completion proposals to return for each prompt.

> üí∏Ô∏è **Additional costs**
>
> You will be charged for the *total* number of tokens generated: `n_completions` * `n_tokens`, stay reasonable!


#### `best_of` <span class="param-types">int</span> <span class="param-optional">null</span> <span class="param-warning">‚ö†Ô∏è smaller than n_completions</span>

Among `n_completions`, only return the `best_of` ones. Completions are selected according to how likely they are, summing the
[log-likelihood](/home/concepts#likelihood) over all tokens generated. 

### Sampling

See the [sampling](/home/concepts#sampling) entry for more details.

#### `mode` <span class="param-types">(greedy, topk, nucleus)</span> <span class="param-optional">nucleus</span>

How the model will decide which token to select at each step. 
* **Greedy**: the model will always select the most likely token. This generation mode is deterministic and only
suited for applications in which there is a ground truth the model is expected to return (e.g. question answering). 
* **Nucleus**: the model will only consider the most likely tokens with total probability mass `p`. We recommend this
    setting for most applications.
* **Top-k**: the model will only consider the `k` most likely tokens.

<<<<<<< HEAD:docs/api/primitives/create.md
Note that the default sampling method for `lyra-fr` is `topk`.

#### `temperature` <span style="color:DimGray">float</span> <span style="color:Gray">1.</span> <span style="color:orange">‚ö†Ô∏è only in topk/nucleus mode</span>
=======
#### `temperature` <span class="param-types">float</span> <span class="param-optional">1.</span> <span class="param-warning">‚ö†Ô∏è only in topk/nucleus mode</span>
>>>>>>> parent of 81578361... Change parameters style:api/primitives/create.md

How risky will the model be in its choice of tokens. A temperature of 0 corresponds to greedy sampling; we recommend 
a value around 1 for most creative applications, and closer to 0 when a ground truth exists.

#### `p` <span class="param-types">float</span> <span class="param-optional">0.9</span> <span class="param-warning">‚ö†Ô∏è only in nucleus mode</span>

Total probability mass of the most likely tokens considered when sampling in nucleus mode. 

#### `k` <span class="param-types">int</span> <span class="param-optional">5</span> <span class="param-warning">‚ö†Ô∏è only in topk mode</span>

Number of most likely tokens considered when sampling in top-k mode. 

### Control

#### `biases` <span class="param-types">map<string, float></span> <span class="param-optional">null</span>

Bias the provided words to appear more or less often in the generated text. 
Values should be comprised between -100 and +100, with negative values making words less likely to occur. Extreme
values such as -100 will completely forbid a word, while values between 1-5 will make the word more likely to appear.
We recommend playing around to find a good fit for your use case.

> üí° **Avoiding repetitions**
>
> When generating longer samples with `biases`, the model may repeat too often positively biased words. Combine
this option with `presence_penalty` and `frequency_penalty` to achieve best results. If you generate a first completion, and then use it as a prompt for a new completion, you probably want to turn off the word bias encouraging a certain word once it has been produced to avoid too much repetition.



> ‚öôÔ∏è **Technical details**
>
>The provided bias is directly added to the [log-likelihood](/home/concepts#likelihood) predicted by the model at a given step, before performing
the sampling operation. You can use the `top_logprobs` option or the **Analyse** endpoint to access the [log-probabilities](/home/concepts#likelihood)
of samples and get an idea of the range of likelihood values in your specific use case.
>
>The bias is actually applied at the token level, and not at the word level. For words made of multiple tokens, the 
bias only applies to the first token (and may thus impact other words). 


#### `presence_penalty` <span class="param-types">float</span> <span class="param-optional">0.</span>

How strongly should tokens be prevented from appearing again. 
This is a one-off penalty: tokens will be penalized after their first appearance, but not more if they appear repetitively 
-- use `frequency_penalty` if that's what you want instead. Use values between 0 and 1. Values closer to 1 encourage variation of the topics generated.

> ‚öôÔ∏è **Technical details**
>
>Once a token appears at least once, `presence_penalty` will be removed from its [log-likelihood](/home/concepts#likelihood) in the future. 


#### `frequency_penalty` <span class="param-types">float</span> <span class="param-optional">0.</span>

How strongly should tokens be prevented from appearing again if they have appeared repetitively. Contrary to `presence_penalty`,
this penalty scales with how often the token already occurs. Use values between 0 and 1. Values closer to 1 discourage repetition, especially useful in combination with `biases`.

> ‚öôÔ∏è **Technical details**
>
>`frequency_penalty` * `n_T` will be removed from the [log-likelihood](/home/concepts#likelihood) of a token, where `n_T`
is the number of occurences of this token in the text already.


#### `stop_words` <span class="param-types">array[string]</span> <span class="param-optional">null</span>

Encountering any of these words will halt generation immediately.

### Utilities

#### `concat_prompt` <span class="param-types">boolean</span> <span class="param-optional">false</span>

The original `prompt` will be concatenated with the generated text in the returned response. 

#### `return_logprobs` <span class="param-types">bool</span> <span class="param-optional">false</span>

Returns the [log-probabilities](/home/concepts#likelihood) of the generated tokens.  

#### `seed` <span class="param-types">int</span> <span class="param-optional">null</span>

Make sampling deterministic by setting a seed used for random number generation. 
Useful for strictly reproducing **Create** calls.

### Skills

#### `skill` <span class="param-types">string</span> <span class="param-optional">null</span>

Specify a ü§π **[Skill](/api/skills)** to use to perform a specific task or to tailor the generated text. 

## Response (`outputs`)

An array of outputs shaped like your batch.

#### `input_text` <span class="param-types">string</span>

The `text` used to generate the text.

### Completions (`completions`)

One entry for each `n_completions` requested. 

#### `output_text` <span class="param-types">string</span>

Text generated by the model. May be concatenated with the `input_text` if `concat_prompt=True`.

#### `score` <span class="param-types">float</span>

Total sum of the [log-probabilities](/home/concepts#likelihood) of the words generated, the higher the better.

#### `normalized_score` <span class="param-types">float</span>

Total sum of the [log-probabilities](/home/concepts#likelihood) of the words generated normalized by the length of the
generated text, the higher the better.

#### `token_scores` <span class="param-types">map<string, float></span>

[Log-probability](/home/concepts#likelihood) of each token generated in the completion, the higher the better.


> ‚öôÔ∏è **Token representations**
>
> Tokens are currently returned as they are represented by the tokenizer, which includes special characters such as `ƒ†`
for spaces and possible encoding oddities (such as `√É¬©` for `√©`). 