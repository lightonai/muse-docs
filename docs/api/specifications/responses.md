---
---

# ðŸ’¬ Responses

To an HTTP request, the API returns all results formatted in **JSON**.

## Response format

The response is made of three parts:
* A **`request_id`**, uniquely identifying the request;
* An **`outputs`** list, containing the model answer to your request, and useful metadata;
* An **`execution_metadata`** dictionary, with the detailed cost of the request. 
Let's have a detailed look at the request example generated on the **[Requests](/api/specifications/requests)** page.

```json title="Response to example request (JSON)"
{
   "request_id":"969718f5-d1f4-40cd-a872-a6fb7bf84329",
   "outputs":[
      [
         {
            "input_text":"Il Ã©tait une fois",
            "completions":[
               {
                  "output_text":" une toute petite fille, qui avait une bonne voix, et chantait des chansons amÃ©ricaines telles que Tabernacle et We all take",
                  "score":-82.67915979400277,
                  "normalized_score":-3.307166391760111,
                  "token_scores":null,
                  "execution_metadata":{
                     "cost":25
                  }
               }
            ],
            "execution_metadata":{
               "cost":25
            }
         }
      ]
   ],
   "total_cost":25
}
```

## Common structures

### Execution metadata

The **`execution_metadata`** dictionary collects information relevant to the **cost and execution of the request**. It is 
available at the top level, as well as for each individual element of a batch. It contains a **`cost`** entry with: 
- **`tokens_used`**, the number of tokens used;
- **`tokens_input`**, the number of tokens sent in input to the model;
- **`tokens_generated`**, the number of tokens generated by the model;
- **`cost_type`** in the form `model_name@skill`, indicating the nature of the tokens used (if no skills are used, it will
be replaced by `default`);
- **`batch_size`** the number of requests made in a single batch;
and a **`finish_reason`** entry, explaining why did the model stopped processing further tokens (`length` if stopped by `n_tokens` or by reaching
the end of the text to process, or `stop_word` if reached one of the `stop_words`).

### Score 

The **`score`** dictionary provides information regarding the log-probabilities of the tokens processed:
- **`logprob`** is the overall log-probability of the entire text processed;
- **`normalized_logprob`** is the same as above, but normalized for text length (number of tokens);
- **`token_logprobs`** is a dictonary including the specific log-probability of each token.

## Response to batched requests

The `outputs` list will be structured according to how you have batched your request: 
* It will contain **one separate list for each set of parameters** you have submitted;
* Each list will contain **one entry per entry in your batch**. 
