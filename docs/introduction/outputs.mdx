import { Alpha, Beta } from '@site/src/components/labels';

# üéØ Outputs

The likelihood of a word represents how likely this word is given previous words, according to the model. For instance, the likelihood of Paris in *"The capital of France is Paris"* is very high, 
whereas the likelihood of *"London"* would be lower, and that of *"book"* would be even smaller.

The log-probability is a representation of the likelihood, ranging from minus infinity to zero. Words with log-probability close to zero have high likelihood, 
whereas words with large negative log-probability (e.g. -10, -50, etc.) are more unlikely. Log-probabilities are also called log-probs or scores as an abbreviation.

They are useful because log-probabilities can simply be added to evaluate the log-probability of a combination of words. 
For instance, the log-prob of "New York" in the sentence "I love New York" is the log-prob of "New" in "I love New" plus the log-prob of "York" in "I love New York". 
This can be used to score entire sentences, and evaluate multiple pre-defined options according to their likelihood.

See our entries about [‚ù§Ô∏è Reviews Classification](/usecases/english/review_classification) or 
[üîéÔ∏è Content Marketing and Search Engine Optimization](/usecases/english/seo) with Muse to learn more. 
For the French version, you can visit [‚ù§Ô∏è Classification de Critiques](/usecases/french/review_classification).

## Embeddings

*Status:* <Alpha />

Embeddings are a numerical representation of a given text, built by the model internally to make predictions. They are a vector (i.e. list) of numbers that encodes 
information about the input text, its context, as well as general knowledge derived from the training data in a computer-understandable format. They can be used by machine learning 
algorithms as a representation of the input text, to compare different sentences and documents, classify samples, or cluster texts.
